---
title: "Homework 2"
subtitle: "ECON 470, Spring 2025"
author: "Molly Catlin"
format:
  pdf:
    output-file: "catlin_molly_hwk2_fs"
    output-ext: "pdf"
    header-includes:
      - \usepackage{float}
      - \floatplacement{table}{H}
---


```{r}
#| include: false
#| eval: false

if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, ggplot2, dplyr, lubridate, readr, readxl, hrbrthemes, fixest,
               scales, gganimate, gapminder, gifski, png, tufte, plotly, OECD,
               ggrepel, survey, foreign, devtools, pdftools, kableExtra, modelsummary,
               kableExtra)
```


Here is a link to my repository: {https://github.com/mollyjc02/Homework_2.git}


\newpage



```{r}
#| include: false

####load("C:/Users/Molly/OneDrive - Emory University/Documents/GitHub/Homework_2/final_submission/Hwk2_workspace.RData")

library(here)
here()

# Load the data using a relative path
load(here("final_submission/Hwk2_workspace.RData")) 
```




## 1. How many hospitals filed more than one report in the same year? Show your answer as a line graph of the number of hospitals over time.


```{r}
#| echo: false
#| label: duplicate-hospitals
#| tbl-cap: Hospitals that Filed Multiple Reports
library(ggplot2)

ggplot(multi_report_counts, aes(x = fyear, y = num_hospitals)) +
geom_line(color = "blue", linewidth  = 1) +
geom_point(color = "red", size  = 2) +
labs(
title = "Hospitals Filing Multiple Reports Per Year",
x = "Year",
y = "Number of Hospitals",
caption = "Source: HCRIS Data (1996 & 2010 Versions)"
) +
theme_minimal()
```


\newpage

## 2. After removing/combining multiple reports, how many unique hospital IDs (Medicare provider numbers) exist in the data?


```{r}
#| echo: false
#| label: unique-hospitals 
#| tbl-cap: Unique Hospitals 

print(fig.unique)
```


\newpage

## 3. What is the distribution of total charges (tot_charges in the data) in each year?


```{r}
#| echo: false
#| label: charge-distributions
#| tbl-cap: Charge Distribution by Year

ggplot(charge.data, aes(x = factor(year), y = tot_charges)) +
  geom_violin(fill = "lightblue", color = "darkblue", alpha = 0.6) +
  scale_y_log10(labels = scales::comma) + 
  labs(
    title = "Distribution of Total Charges by Year",
    x = "Year",
    y = "Total Charges (log scale)",
    caption = "Source: HCRIS Data (1996 & 2010 Versions)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 70, hjust = 1))
```


\newpage

## 4. What is the distribution of estimated prices in each year?

```{r}
#| echo: false
#| label: price-distributions
#| tbl-cap: Estimated Price Distribution by Year


ggplot(price.data, aes(x = as.factor(year), y = price)) +
  geom_violin(trim = TRUE, fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of Estimated Prices by Year",
    x = "Year",
    y = "Estimated Price"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 70, hjust = 1))
```



\newpage

# For the rest of the assignment, I have included only observations in 2012. So I am now dealing with cross-sectional data in which some hospitals are penalized and some are not.


## 5. Calculate the average price among penalized versus non-penalized hospitals.

The average price among penalized hospitals is `r mean.pen`, while the average price among non-penalized hospitals is `r mean.nopen`. 

\newpage

## 6. Split hospitals into quartiles based on bed size and provide a table of the average price among treated/control groups for each quartile.

```{r}
#| echo: false
#| label: price-by-quartile
#| tbl-cap: Average Price by Treatment Status and Bed Size 

knitr::kable(
  quartile_summary,
  col.names = c("Bed Quartile", "No Penalty", "Penalty"),
  caption = "Average Prices by Bed Quartile and Penalty Status",
  format = "markdown"
)
```


\newpage


## 7. Find the average treatment effect based on quartiles of bed size using each of the following estimators: nearest neighbor matching with inverse variance distance, nearest neighbor matching with Mahalanobis distance, inverse propensity weighting, and simple linear regression.

```{r}
#| echo: false
#| label: ate-table
#| tbl-cap: ATE Estimates

# Extract ATE estimates
ate_near_match <- near.match$est
ate_mal_match <- mal.match$est
ate_ipw_diff <- ipw.diff
ate_regression <- ate


# Create a data frame summarizing the results
ate_estimates <- data.frame(
  Method = c(
    "Nearest Matching (Inverse Variance)",
    "Nearest Matching (Mahalanobis Distance)",
    "Inverse Propensity Weighting (IPW)",
    "Linear Regression"
  ),
  ATE_Estimate = c(ate_near_match, ate_mal_match, ate_ipw_diff, ate_regression)
)


# Print the table using knitr::kable()
knitr::kable(ate_estimates,
             caption = "ATE Estimates from Different Methods",
             col.names = c("Method", "ATE Estimate"))
```


\newpage

## 8. With these different treatment effect estimators, are the results similar, identical, very different?
Using nearest neighbor matching with inverse variance distance, nearest neighbor matching with Mahalanobis distance, inverse propensity weighting, and simple linear regression as estimators based on quartiles of bed size, I got identical results for the average treatment effect of instilling penalties. 

## 9. Do you think you’ve estimated a causal effect of the penalty? Why or why not?
No, I do not think I've estimated a causal effect of the penalty. Although the estimating techniques used (such as matching and weighting) do improve the validity of relationships being measured, these results are still based on observational data and thus cannot determine causality. Additionally, in this data, I only looked into the estimated effect based on quartiles of bed size. There are a number of other unincluded confounders that could be influencing both the likelihood of receiving a penalty and the hospital’s pricing behavior. So, while matching techniques do help control for confounders in the data, matching on only one variable is not sufficient to claim that the estimated effect is causal. 

## 10. Briefly describe your experience working with these data (just a few sentences). Tell me one thing you learned and one thing that really aggravated or surprised you.
In working with HCRIS data, I found the process of data cleaning to be extremely frustrating. Particularly, I struggled to know when it was appropriate to impose restrictions (e.g., cap the data at the 99th percentile) and how aggressive to be with these impositions (e.g., capping results at the 99th versus 95th percentile). That said, I did learn a lot about how to best structure my data cleaning and realized that it is really important to keep track of alterations made to variables in the data, which I think will help prevent me from running into similar issues in future assignments. Additionally, although not specific to this data necessarily, I found that forming my Quarto document was very frustrating. I spent nearly as much time figuring out why my Quarto wasn't rendering and making the document look presentable as I did on writing my code.However, I do feel I have now learned how to better and more efficiently work within Quarto, as well as how to interpret the errors I receive, which will definitely be a useful skillset to have.